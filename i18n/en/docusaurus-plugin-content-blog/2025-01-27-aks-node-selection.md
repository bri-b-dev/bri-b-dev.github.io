---
slug: aks-node-selection
title: ‚ÄúAKS Node Selection: Physical Pools vs. Virtual Nodes‚Äù
authors: brigitte
tags: [kubernetes, aks, azure, nodepool, virtual-node, scheduling]
date: 2025-01-27
description: ‚ÄúStrategies for running workloads in AKS preferentially on physical user nodes ‚Äì with automatic fallback to virtual nodes.‚Äù
---

import Admonition from '@theme/Admonition';

In many projects, the **cost and resource model** is crucial:
- Physical AKS node pools (`user nodes`) are cheaper and optimized for continuous workloads.  
- **Virtual nodes** (based on Azure Container Instances) are ideal for **burst scenarios**‚Äîwhen more capacity is needed at short notice.  
<!--truncate-->
üëâ Goal: Workloads should **always use the physical nodes first**, but automatically switch to virtual nodes when there are no more resources available there.

---

## ‚öôÔ∏è Basics: Node pools in AKS
- **System Pool**: internal AKS services
- **User Pool**: physical VM-based nodes (e.g., VMSS with Standard_D4s_v5)
- **Virtual Node Pool**: based on ACI, highly scalable, pay-per-use, no VM instance costs

---

## üöß Challenge
By default, Kubernetes distributes pods evenly ‚Äì without ‚Äúpreference.‚Äù  
If you want to use virtual nodes **only as a stopgap measure**, you need a clean scheduling strategy.

---

## ‚úÖ Strategies for Node Selection

### 1. NodeSelector + Taints/Tolerations
- User nodes: no special taints ‚Üí pods run here by default.  
- Virtual nodes: tainted (`virtual-kubelet.io/provider=azure:NoSchedule`).  
- Only pods that set **tolerations** are allowed to fall back to virtual nodes.

```yaml
tolerations:
  - key: ‚Äúvirtual-kubelet.io/provider‚Äù
    operator: ‚ÄúEqual‚Äù
    value: ‚Äúazure‚Äù
    effect: ‚ÄúNoSchedule‚Äù
````

‚û°Ô∏è Advantage: full control, default = user nodes, virtual nodes = fallback.

---

2. Affinity & Preferred Scheduling

`nodeAffinity` can be used to express a **preference**:

* ‚ÄúPrefer user nodes‚Äù (preferred)
* ‚ÄúAllow virtual nodes‚Äù (soft)

```yaml
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: kubernetes.azure.com/mode
              operator: In
              values:
                - user
```

üëâ Only when there is no more space there will pods be distributed to other nodes (including virtual nodes).

---

### 3. Workload-specific control

* **Batch/burst jobs**: Set `tolerations` so that they can use virtual nodes.
* **Persistent services**: No taint/toleration ‚Üí remain strictly on physical nodes.

---

## üìä Visualization: Scheduling Strategy

```mermaid
flowchart TD
    subgraph AKS[‚ÄúAKS Cluster‚Äù]
        subgraph UserPool[‚ÄúUser Node Pool (VMs)‚Äù]
            U1[‚ÄúUser Node 1‚Äù]
            U2[‚ÄúUser Node 2‚Äù]
            U3[‚ÄúUser Node 3‚Äù]
        end

        subgraph VirtualPool[‚ÄúVirtual Node Pool (ACI)‚Äù]
            V1[‚ÄúVirtual Node‚Äù]
        end

        P1[‚ÄúPod A (Deployment)‚Äù]
        P2[‚ÄúPod B (Job)‚Äù]
    end

    P1 -->|preferred| U1 & U2 & U3
    P1 -.->|fallback| V1

    P2 -->|toleration| V1
```

* **Pod A (Deployment)**: prefers user nodes, but falls back to virtual nodes when resources are scarce.
* **Pod B (Job)**: has explicit tolerance ‚Üí may run directly on virtual nodes.

---

## üìå Best Practices

* **Monitoring**: Track exactly how many pods are running on virtual nodes (cost control).
* **SLA**: Virtual nodes have different limits (no DaemonSet support, limited features).
* **Workload design**: Short jobs and burst-like loads ‚Üí virtual nodes; critical systems ‚Üí user nodes.
* **Cost model**: Physical pools for base load, Virtual Nodes only for peaks.

<Admonition type="tip" title="Avoid cost traps">
Set limits and autoscaling correctly, otherwise too many pods will end up permanently on expensive Virtual Nodes!
</Admonition>

---

## üìå Conclusion

With **Affinity, Taints & Tolerations**, two-stage scheduling can be implemented:

* Permanent workloads run reliably and cost-effectively on physical user nodes.
* Peak loads are automatically directed to virtual nodes ‚Äì flexibly, scalably, and without overprovisioning.

---

## üì¶ Example manifests (deployment & batch job)

> Assumptions:
>
> * **User nodes** carry the label: `kubernetes.azure.com/mode=user`
> * **Virtual nodes** are tainted with `virtual-kubelet.io/provider=azure:NoSchedule`
> * Cluster has at least one Linux virtual node (ACI)

### 1) Deployment: prefers user nodes, fallback to virtual node

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  labels:
    app: web-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      # ‚ù∂ Prefer physical user nodes
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: kubernetes.azure.com/mode
                    operator: In
                    values: ["user"]
      # ‚ù∑ Allow fallback to virtual nodes (tolerate taint)
      tolerations:
        - key: "virtual-kubelet.io/provider"
          operator: "Equal"
          value: "azure"
          effect: "NoSchedule"
      # ‚ù∏ Optional: Distribute pods across user nodes (cost & resilience)
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: web-frontend
      containers:
        - name: app
          image: ghcr.io/example/web:1.2.3
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "250m"
              memory: "256Mi"
            limits:
              cpu: "1"
              memory: "512Mi"
```

> Result: As long as resources are available on user nodes, all replicas are placed there. Only when resources are scarce can pods also be scheduled on virtual nodes thanks to **tolerance**.

---

### 2) BatchJob: prefers virtual nodes to conserve user pool

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: image-transcode
spec:
  completions: 5
  parallelism: 5
  backoffLimit: 0
  template:
    spec:
      # ‚ù∂ Prefer Virtual Node (soft), but allow users as backup
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: kubernetes.io/role
                    operator: In
                    values: ["virtual-node"]
      # ‚ù∑ Tolerance for the virtual node taint (required for scheduling)
      tolerations:
        - key: "virtual-kubelet.io/provider"
          operator: "Equal"
          value: "azure"
          effect: "NoSchedule"
      restartPolicy: Never
      containers:
        - name: worker
          image: ghcr.io/example/transcoder:2.0.0
          args: ["--input", "$(INPUT)", "--output", "$(OUTPUT)"]
          env:
            - name: INPUT
              value: "/data/in"
            - name: OUTPUT
              value: "/data/out"
          resources:
            requests:
              cpu: "1"
              memory: "1Gi"
            limits:
              cpu: "2"
              memory: "2Gi"
```

> Note: The key `kubernetes.io/role=virtual-node` is **an example label**. In many clusters, a suitable label already exists on virtual nodes (e.g., `type=virtual-kubelet` or `kubernetes.azure.com/virtual-node=true`). Adjust the **match expression** to your actual node labels.

---

### 3) Variant: Strict separation via NodeSelector

If certain workloads should **never** run on virtual nodes, use a hard `nodeSelector` on user nodes **without** tolerations:

```yaml
spec:
  template:
    spec:
      nodeSelector:
        kubernetes.azure.com/mode: ‚Äúuser‚Äù
      # No toleration ‚Üí no scheduling on virtual nodes possible
```

And vice versa (virtual node only):

```yaml
spec:
  template:
    spec:
      tolerations:
        - key: "virtual-kubelet.io/provider"
          operator: "Equal"
          value: "azure"
          effect: "NoSchedule"
      nodeSelector:
        kubernetes.azure.com/virtual-node: "true" # Sample label, customize it for your needs
```

---

### 4) Horizontal Pod Autoscaler (HPA) as a burst trigger

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-frontend
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-frontend
  minReplicas: 3
  maxReplicas: 30
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```