"use strict";(self.webpackChunkbri_b_dev_github_io=self.webpackChunkbri_b_dev_github_io||[]).push([[5418],{930:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"field-filtering-rest-jackson","metadata":{"permalink":"/en/blog/field-filtering-rest-jackson","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025-02-17-field-filtering.md","title":"Field-Filtering in REST-APIs with Jackson & @ControllerAdvice","description":"Dynamic reduction of response fields via query parameters \u2013 an elegant solution with mixins and MappingJacksonValue.","date":"2025-02-17T00:00:00.000Z","tags":[{"inline":false,"label":"Spring Boot","permalink":"/en/blog/tags/spring-boot","description":"Spring Boot"},{"inline":false,"label":"Kotlin","permalink":"/en/blog/tags/kotlin","description":"Kotlin"},{"inline":false,"label":"Java","permalink":"/en/blog/tags/java","description":"Java"},{"inline":false,"label":"REST","permalink":"/en/blog/tags/rest","description":"REST"},{"inline":false,"label":"Jackson","permalink":"/en/blog/tags/jackson","description":"Jackson"},{"inline":false,"label":"Json","permalink":"/en/blog/tags/json","description":"JavaScript Object Notation"}],"readingTime":1.81,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"field-filtering-rest-jackson","title":"Field-Filtering in REST-APIs with Jackson & @ControllerAdvice","authors":"brigitte","tags":["spring-boot","kotlin","java","rest","jackson","json"],"date":"2025-02-17T00:00:00.000Z","description":"Dynamic reduction of response fields via query parameters \u2013 an elegant solution with mixins and MappingJacksonValue."},"unlisted":false,"nextItem":{"title":"API-First wotj Spring Boot & Kotlin","permalink":"/en/blog/api-first-springboot-kotlin"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nClients often do not want to receive **all fields** of a REST response.  \\nExamples:\\n- Mobile apps only need `id` and `name`, not the complete DTO.  \\n- Analytics systems only want certain metrics.  \\n\x3c!--truncate--\x3e\\nInstead of building multiple endpoints, you can implement **field filtering via query parameters**:  \\n`GET /api/spaces?fields=id,name`\\n\\n---\\n\\n## \u2699\ufe0f Setup\\n\\nWe use:\\n- **Spring Boot (Kotlin/Java)**  \\n- **Jackson @JsonFilter + Mixins**  \\n- **@ControllerAdvice**, `MappingJacksonValue` returns  \\n\\n---\\n\\n## \ud83d\udd17 Example: DTOs & Controller\\n\\n**`SpaceReadDTO.kt`**\\n```kotlin\\ndata class SpaceReadDTO(\\n    val id: UUID,\\n    val name: String,\\n    val description: String,\\n    val createdAt: Instant,\\n    val owner: String\\n)\\n````\\n\\n**`SpaceController.kt`**\\n\\n```kotlin\\n@RestController\\n@RequestMapping(\\"/api/spaces\\")\\nclass SpaceController {\\n\\n    @GetMapping\\n    fun getSpaces(): List<SpaceReadDTO> =\\n        listOf(\\n            SpaceReadDTO(UUID.randomUUID(), \\"Alpha\\", \\"First Space\\", Instant.now(), \\"Brigitte\\"),\\n            SpaceReadDTO(UUID.randomUUID(), \\"Beta\\", \\"Second Space\\", Instant.now(), \\"Alex\\")\\n        )\\n}\\n```\\n\\n\ud83d\udc49 Still without filtering.\\n\\n---\\n\\n## \ud83e\ude84 Field-Filter Advice\\n\\nWe write a **@ControllerAdvice** that intercepts responses and reduces fields if necessary:\\n\\n**`FieldFilterAdvice.kt`**\\n\\n```kotlin\\n@ControllerAdvice\\nclass FieldFilterAdvice(val objectMapper: ObjectMapper) : ResponseBodyAdvice<Any> {\\n\\n    override fun supports(\\n        returnType: MethodParameter,\\n        converterType: Class<out HttpMessageConverter<*>>\\n    ) = true\\n\\n    override fun beforeBodyWrite(\\n        body: Any?,\\n        returnType: MethodParameter,\\n        contentType: MediaType,\\n        converterType: Class<out HttpMessageConverter<*>>,\\n        request: ServerHttpRequest,\\n        response: ServerHttpResponse\\n    ): Any? {\\n        if (body == null) return null\\n\\n        val servletRequest = (request as? ServletServerHttpRequest)?.servletRequest\\n        val fieldsParam = servletRequest?.getParameter(\\"fields\\") ?: return body\\n\\n        val fields = fieldsParam.split(\\",\\").map { it.trim() }.toSet()\\n        if (fields.isEmpty()) return body\\n\\n        // Dynamic Filter-Setup\\n        val filterId = \\"dynamicFilter\\"\\n        objectMapper.setFilterProvider(\\n            SimpleFilterProvider().addFilter(\\n                filterId,\\n                SimpleBeanPropertyFilter.filterOutAllExcept(fields)\\n            )\\n        )\\n\\n        // Mixin with @JsonFilter\\n        val targetClass = body.javaClass\\n        objectMapper.addMixIn(targetClass, DynamicFilterMixin::class.java)\\n\\n        return MappingJacksonValue(body).apply { filters = objectMapper.serializationConfig.filterProvider }\\n    }\\n\\n    @JsonFilter(\\"dynamicFilter\\")\\n    class DynamicFilterMixin\\n}\\n```\\n\\n---\\n\\n## \ud83d\ude80 Result\\n\\nCall without parameter:\\n\\n```http\\nGET /api/spaces\\n```\\n\\nResponse:\\n\\n```json\\n[\\n  { \\"id\\": \\"\u2026\\", \\"name\\": \\"Alpha\\", \\"description\\": \\"First Space\\", \\"createdAt\\": \\"\u2026\\", \\"owner\\": \\"Brigitte\\" }\\n]\\n```\\n\\nCall with filter:\\n\\n```http\\nGET /api/spaces?fields=id,name\\n```\\n\\nResponse:\\n\\n```json\\n[\\n  { \\"id\\": \\"\u2026\\", \\"name\\": \\"Alpha\\" }\\n]\\n```\\n\\n---\\n\\n## \u2705 Lessons Learned\\n\\n* Works for individual objects **and** lists.\\n* `fields` parameter can be combined flexibly (`id,name,owner`).\\n* Multiple DTOs \u2192 use your own filter IDs and mixins if necessary.\\n* Be careful with **nested objects** \u2013 field filtering only works at the top level.\\n\\n<Admonition type=\\"tip\\" title=\\"Pro tip\\">\\nBuild helper methods for frequently used field sets, e.g., `?fields=summary` \u2192 expands into specific fields.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nWith `@ControllerAdvice`, Jackson filters, and `MappingJacksonValue`, **field filtering can be implemented elegantly and generically**.\\nThis saves you boilerplate endpoints and returns exactly the data that clients really need."},{"id":"api-first-springboot-kotlin","metadata":{"permalink":"/en/blog/api-first-springboot-kotlin","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025-02-10-api-first.md","title":"API-First wotj Spring Boot & Kotlin","description":"How I design microservices according to OpenAPI and automatically generate clients & server stubs \u2013 Lessons learned in Kotlin.","date":"2025-02-10T00:00:00.000Z","tags":[{"inline":false,"label":"Spring Boot","permalink":"/en/blog/tags/spring-boot","description":"Spring Boot"},{"inline":false,"label":"Kotlin","permalink":"/en/blog/tags/kotlin","description":"Kotlin"},{"inline":false,"label":"OpenAPI","permalink":"/en/blog/tags/openapi","description":"OpenAPI"},{"inline":false,"label":"Microservices","permalink":"/en/blog/tags/microservices","description":"Microservices"},{"inline":false,"label":"Codegen","permalink":"/en/blog/tags/codegen","description":"Codegen"},{"inline":false,"label":"API-first","permalink":"/en/blog/tags/api-first","description":"API-first"}],"readingTime":2.94,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"api-first-springboot-kotlin","title":"API-First wotj Spring Boot & Kotlin","authors":"brigitte","tags":["spring-boot","kotlin","openapi","microservices","codegen","api-first"],"date":"2025-02-10T00:00:00.000Z","description":"How I design microservices according to OpenAPI and automatically generate clients & server stubs \u2013 Lessons learned in Kotlin."},"unlisted":false,"prevItem":{"title":"Field-Filtering in REST-APIs with Jackson & @ControllerAdvice","permalink":"/en/blog/field-filtering-rest-jackson"},"nextItem":{"title":"Terraform Patterns for AKS & Azure","permalink":"/en/blog/terraform-patterns-aks-azure"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nMicroservices should offer **consistent interfaces** \u2013 regardless of the programming language.\\nAPI-first means: **first the OpenAPI schema**, then code, documentation, and clients.\\n\x3c!--truncate--\x3e\\nThis prevents API definitions from becoming outdated or being added as an \u201cafterthought.\\"\\n\\n---\\n\\n## \ud83e\udde9 Workflow: API-First in practice\\n\\n1. **Design**: OpenAPI spec (`.yaml`) with Stoplight, Swagger Editor, or VS Code plugin.\\n2. **Validate**: Linter (e.g., Spectral) and CI checks.\\n3. **Codegen**: Generate server stubs & client SDKs from the spec.\\n4. **Implement**: Business logic in Kotlin, interface remains stable.\\n5. **Docs**: Swagger UI or ReDoc from the same spec.\\n\\n---\\n\\n## \u2699\ufe0f Setup: OpenAPI Generator in Kotlin\\n\\n**`build.gradle.kts`**\\n\\n```kotlin\\nplugins {\\n    id(\\"org.springframework.boot\\") version \\"3.3.0\\"\\n    id(\\"io.spring.dependency-management\\") version \\"1.1.5\\"\\n    kotlin(\\"jvm\\") version \\"1.9.24\\"\\n    kotlin(\\"plugin.spring\\") version \\"1.9.24\\"\\n    id(\\"org.openapi.generator\\") version \\"7.5.0\\"\\n}\\n\\nopenApiGenerate {\\n    generatorName.set(\\"kotlin-spring\\")\\n    inputSpec.set(\\"$rootDir/api/openapi.yaml\\")\\n    outputDir.set(\\"$buildDir/generated\\")\\n    apiPackage.set(\\"com.example.api\\")\\n    modelPackage.set(\\"com.example.api.model\\")\\n    configOptions.set(\\n        mapOf(\\n            \\"interfaceOnly\\" to \\"true\\",\\n            \\"useSpringBoot3\\" to \\"true\\",\\n            \\"reactive\\" to \\"false\\"\\n        )\\n    )\\n}\\n```\\n\\n\ud83d\udc49 Result:\\n\\n* **API interfaces** as Kotlin interfaces (controller skeletons)\\n* **DTOs** in the `model` package\\n\\n---\\n\\n## \ud83d\udea7 Lessons Learned\\n\\n### 1. Interface-only is worth its weight in gold\\n\\n* Only generate interfaces; implementation remains **clean in your own code**.\\n* No merge conflicts when regenerating.\\n\\n### 2. Use DTOs strictly\\n\\n* DTOs from `model` are **transport objects**\u2014not business entities.\\n* Avoid \u201csmart\\" logic in DTOs, keep them flat.\\n\\n### 3. Versioning & backward compatibility\\n\\n* Version OpenAPI as a **single source of truth** in Git.\\n* Breaking changes only with new API version (`/v2/...`).\\n\\n### 4. CI/CD checks\\n\\n* Validate OpenAPI spec in every PR (`spectral lint`).\\n* Check codegen in CI to ensure everything is up to date (`git diff` on generated code).\\n\\n---\\n\\n## \ud83d\udce6 Client generation\\n\\nOpenAPI Generator can also be used to create **clients** for other languages:\\n\\n```bash\\nopenapi-generator-cli generate \\\\\\n  -i api/openapi.yaml \\\\\\n  -g typescript-fetch \\\\\\n  -o clients/ts\\n```\\n\\n* Frontend uses `typescript-fetch` or `typescript-axios`.\\n* Other services can obtain `java`, `python`, `go` SDKs.\\n* Uniform API guarantees consistency across the stack.\\n\\n---\\n\\n## \ud83d\udd17 Mini example: User API end-to-end\\n\\n### 1. OpenAPI Spec (abridged)\\n\\n**`api/openapi.yaml`**\\n\\n```yaml\\nopenapi: 3.0.3\\ninfo:\\n  title: User API\\n  version: 1.0.0\\npaths:\\n  /users/{id}:\\n    get:\\n      summary: Get user by ID\\n      parameters:\\n        - in: path\\n          name: id\\n          required: true\\n          schema:\\n            type: string\\n            format: uuid\\n      responses:\\n        \'200\':\\n          description: OK\\n          content:\\n            application/json:\\n              schema:\\n                $ref: \'#/components/schemas/User\'\\n        \'404\':\\n          description: Not Found\\ncomponents:\\n  schemas:\\n    User:\\n      type: object\\n      properties:\\n        id:\\n          type: string\\n          format: uuid\\n        name:\\n          type: string\\n```\\n\\n### 2. Generated interface (abridged)\\n\\n**`build/generated/com/example/api/UsersApi.kt`**\\n\\n```kotlin\\npackage com.example.api\\n\\nimport com.example.api.model.User\\nimport org.springframework.http.ResponseEntity\\nimport org.springframework.web.bind.annotation.PathVariable\\n\\ninterface UsersApi {\\n    fun getUserById(@PathVariable(\\"id\\") id: java.util.UUID): ResponseEntity<User>\\n}\\n```\\n\\n### 3. Kotlin implementation\\n\\n**`src/main/kotlin/com/example/controller/UserController.kt`**\\n\\n```kotlin\\npackage com.example.controller\\n\\nimport com.example.api.UsersApi\\nimport com.example.api.model.User\\nimport org.springframework.http.ResponseEntity\\nimport org.springframework.web.bind.annotation.RestController\\nimport java.util.UUID\\n\\n@RestController\\nclass UserController : UsersApi {\\n    override fun getUserById(id: UUID): ResponseEntity<User> {\\n        val user = if (id.toString().startsWith(\\"a\\")) {\\n            User(id = id.toString(), name = \\"Alice\\")\\n        } else null\\n\\n        return user?.let { ResponseEntity.ok(it) }\\n            ?: ResponseEntity.notFound().build()\\n    }\\n}\\n```\\n\\n\ud83d\udc49 Flow: **Spec \u2192 Codegen \u2192 Interface \u2192 Implementation**. All teams use the same API definition.\\n\\n---\\n\\n## \ud83d\udccc Best Practices\\n\\n* **Keep the spec small**: Don\'t model everything upfront, but expand incrementally.\\n* **Consistent names**: API paths, models, enums \u2192 consistent naming convention.\\n* **Security**: Define security schemes (OAuth2, Bearer JWT) right in the spec.\\n* **Documentation**: Use Swagger UI directly in the service \u2013 developers get immediate feedback.\\n\\n<Admonition type=\\"tip\\" title=\\"API-First pays off\\">\\nThe extra effort at the beginning pays off many times over: Consistent interfaces, less friction between teams, faster development.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nAPI-first with OpenAPI and Kotlin + Spring Boot means: **interface first, code second**.\\nGenerators for stubs & clients make the process reproducible, the documentation always stays up to date, and microservices can be integrated faster."},{"id":"terraform-patterns-aks-azure","metadata":{"permalink":"/en/blog/terraform-patterns-aks-azure","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025-02-03-terraform-patterns.md","title":"Terraform Patterns for AKS & Azure","description":"Experiences with modular Terraform setups for AKS \u2013 from module design and RBAC to network policies and CI/CD.","date":"2025-02-03T00:00:00.000Z","tags":[{"inline":false,"label":"Terraform","permalink":"/en/blog/tags/terraform","description":"Terraform"},{"inline":false,"label":"AKS","permalink":"/en/blog/tags/aks","description":"Azure Kubernetes Service"},{"inline":false,"label":"Azure","permalink":"/en/blog/tags/azure","description":"Azure"},{"inline":false,"label":"RBAC","permalink":"/en/blog/tags/rbac","description":"Role-Based Access Control"},{"inline":false,"label":"Network Policy","permalink":"/en/blog/tags/network-policy","description":"Network Policy"},{"inline":false,"label":"Modules","permalink":"/en/blog/tags/modules","description":"Modules"},{"inline":false,"label":"CI/CD","permalink":"/en/blog/tags/cicd","description":"Continuous Integration and Continuous Deployment"}],"readingTime":13.48,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"terraform-patterns-aks-azure","title":"Terraform Patterns for AKS & Azure","authors":"brigitte","tags":["terraform","aks","azure","rbac","network-policy","modules","cicd"],"date":"2025-02-03T00:00:00.000Z","description":"Experiences with modular Terraform setups for AKS \u2013 from module design and RBAC to network policies and CI/CD."},"unlisted":false,"prevItem":{"title":"API-First wotj Spring Boot & Kotlin","permalink":"/en/blog/api-first-springboot-kotlin"},"nextItem":{"title":"AKS Node Selection: Physical Pools vs. Virtual Nodes","permalink":"/en/blog/aks-node-selection"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nAKS projects grow quickly: **clusters, node pools, identities, networks, ACR, policies** \u2013 and parameters vary per environment (dev/test/prod). Without structure, the code base becomes fragile. In this post, I\'ll show **proven Terraform patterns** for Azure & AKS from projects, including **RBAC** and **network policy** pitfalls.\\n\x3c!--truncate--\x3e\\n---\\n\\n## \ud83e\uddf1 Module architecture: Separate by responsibilities\\n\\n**Goal:** Reusable, clearly defined modules instead of a \\"monolith.\\"\\n\\n```text\\ninfra/\\n\u251c\u2500 modules/\\n\u2502  \u251c\u2500 network/                 # VNet, Subnets, NSGs, UDR/NAT\\n\u2502  \u251c\u2500 aks/                     # AKS Cluster + Pools\\n\u2502  \u251c\u2500 identity/                # UAMI/MI + Role Assignments\\n\u2502  \u251c\u2500 acr/                     # Container Registry\\n\u2502  \u251c\u2500 monitoring/              # Log Analytics, Insights\\n\u2502  \u251c\u2500 policies/                # Azure Policy + AKS Add-Ons\\n\u2502  \u2514\u2500 dns/                     # Private DNS Zones\\n\u251c\u2500 env/\\n\u2502  \u251c\u2500 dev/\\n\u2502  \u2502  \u251c\u2500 main.tf               # Assembling the modules\\n\u2502  \u2502  \u251c\u2500 variables.tfvars\\n\u2502  \u2502  \u2514\u2500 backend.tf            # Remote State\\n\u2502  \u2514\u2500 prod/\\n\u2502     \u251c\u2500 ...\\n\u2514\u2500 global/\\n   \u2514\u2500 rg.tf                    # Resource groups, tags, management\\n```\\n\\n**Pattern:** Environments are **compositions** of modules. Each module has a **clear API** (inputs/outputs) and minimal side effects.\\n\\n<Admonition type=\\"tip\\" title=\\"Keep Inputs simple\\">\\nAvoid huge, nested `object` variables. It is better to use several flat inputs with defaults \u2013 this reduces `Unknown` diffs and makes upgrades easier.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udd27 Example: AKS module (interface)\\n\\n```hcl\\nvariable \\"name\\" { type = string }\\nvariable \\"location\\" { type = string }\\nvariable \\"resource_group_name\\" { type = string }\\nvariable \\"kubernetes_version\\" { type = string }\\nvariable \\"network_profile\\" {\\n  type = object({\\n    plugin           = string   # azure, kubenet, cni_overlay\\n    pod_cidr         = optional(string)\\n    service_cidr     = string\\n    dns_service_ip   = string\\n    outbound_type    = string   # loadBalancer, userDefinedRouting, managedNATGateway\\n  })\\n}\\nvariable \\"enable_azure_rbac\\" { type = bool, default = true }\\nvariable \\"aad_admin_group_object_ids\\" { type = list(string), default = [] }\\nvariable \\"system_node_pool\\" {\\n  type = object({\\n    name       = string\\n    vm_size    = string\\n    min_count  = number\\n    max_count  = number\\n    os_sku     = optional(string, \\"Ubuntu\\")\\n  })\\n}\\nvariable \\"user_node_pools\\" {\\n  type = list(object({\\n    name       = string\\n    vm_size    = string\\n    min_count  = number\\n    max_count  = number\\n    taints     = optional(list(string), [])\\n    labels     = optional(map(string), {})\\n    mode       = optional(string, \\"User\\")\\n  }))\\n  default = []\\n}\\noutput \\"kubelet_identity_principal_id\\" { value = azurerm_kubernetes_cluster.this.kubelet_identity[0].object_id }\\noutput \\"cluster_id\\" { value = azurerm_kubernetes_cluster.this.id }\\n```\\n\\n> **Note:** Block names/flags differ depending on provider versions. Capsule version specifics **in the module** and provide stable inputs to the outside world.\\n\\n---\\n\\n## \ud83e\udeaa RBAC & Identities: Common Pitfalls\\n\\n### 1) Azure RBAC vs. Kubernetes RBAC\\n\\n* **Azure RBAC for Kubernetes** (\\"AKS-managed AAD\\") simplifies AuthN/AuthZ, but **mapping & propagation** may take seconds/minutes.\\n* **Pattern:** Create **AAD groups** for cluster roles (e.g. `aks-admins`, `aks-devs`) and pass their object IDs as module input.\\n\\n```hcl\\n# Pseudocode \u2013 module uses these IDs\\nvariable \\"aad_admin_group_object_ids\\" { type = list(string) }\\n# In the cluster block: activate AAD/RBAC and register groups as admins\\n```\\n\\n**Anti-pattern:** Storing individual users directly \u2192 difficult to maintain, no rotation.\\n\\n### 2) Kubelet/ACR permissions\\n\\n* To enable nodes to pull images: `AcrPull` on **ACR** for the **Kubelet identity**.\\n* Additionally: Build/Push pipeline \u2192 `AcrPush` for CI service principal or UAMI.\\n\\n```hcl\\nresource \\"azurerm_role_assignment\\" \\"kubelet_acr_pull\\" {\\n  scope                = azurerm_container_registry.acr.id\\n  role_definition_name = \\"AcrPull\\"\\n  principal_id         = module.aks.kubelet_identity_principal_id\\n}\\n```\\n\\n### 3) Network roles\\n\\n* For **UDR/NAT Gateway**: `Network Contributor` on subnet/route table for **AKS-MI** (cluster identity) \u2013 otherwise provisioning/scale will fail.\\n\\n<Admonition type=\\"caution\\" title=\\"Eventual Consistency\\">\\nRole assignments are **eventually consistent**. Plan for wait times/`depends_on` or use a retry wrapper module.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udd10 Network Policies: Practice Instead of Theory\\n\\n**Goal:** Default deny at pod level + targeted allow rules.\\n\\n* **CNI/policy matrix** differs depending on AKS version: Azure CNI (Classic/Overlay) & Kubenet behave differently.\\n* **Pattern:** Parameterize policy engine (`azure`, `calico`) as module input and generate **basic rules** centrally.\\n\\n### Baseline (namespace side) \u2013 Default Deny\\n\\n```yaml\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: default-deny\\n  namespace: myns\\nspec:\\n  podSelector: {}\\n  policyTypes: [Ingress, Egress]\\n```\\n\\n### Allow: Ingress from the ingress controller + DNS egress\\n\\n```yaml\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: allow-ingress-from-gateway\\n  namespace: myns\\nspec:\\n  podSelector:\\n    matchLabels:\\n      app: web\\n  ingress:\\n    - from:\\n        - namespaceSelector:\\n            matchLabels:\\n              kubernetes.io/metadata.name: ingress-nginx\\n  egress:\\n    - to:\\n        - namespaceSelector:\\n            matchLabels:\\n              kubernetes.io/metadata.name: kube-system\\n      ports:\\n        - protocol: UDP\\n          port: 53\\n```\\n\\n<Admonition type=\\"note\\" title=\\"Test\\">\\nValidate policies with `netshoot`, `curl`, `dig`, and CI checks (e.g., Kyverno/OPA constraints). Automated smoke tests are invaluable.\\n</Admonition>\\n\\n---\\n\\n## \ud83c\udf10 Network setup: proven options\\n\\n* **Outbound**: `managedNATGateway` or `userDefinedRouting` with Azure Firewall.\\n* **Private clusters**: Private endpoint + DNS zones, jump host/bastion for `kubectl`.\\n* **Ingress**: AGIC or NGINX; for private clusters \u2192 internal load balancer/private link.\\n* **Egress lockdown**: Azure Firewall DNAT/application rules; policies for prohibited public IPs.\\n\\n**Pattern:** Network module delivers **subnet IDs**/routes as outputs to the AKS module; no circular dependencies.\\n\\n---\\n\\n## \ud83e\uddea Environments, Workspaces & State\\n\\n* **Remote State** in Azure Storage (Blob) with **state locking** via storage lease.\\n* **One workspace per environment** (e.g., `dev`, `prod`) \u2013 no mixing.\\n* **tfvars** per environment + `locals` for derived values (tags, naming conventions, CIDRs).\\n\\n```hcl\\n# backend.tf (je Env)\\nterraform {\\n  backend \\"azurerm\\" {}\\n  required_version = \\">= 1.7.0\\"\\n  required_providers {\\n    azurerm = {\\n      source  = \\"hashicorp/azurerm\\"\\n      version = \\"~> 3.100\\"\\n    }\\n  }\\n}\\n```\\n\\n<Admonition type=\\"tip\\" title=\\"Naming conventions\\">\\nA `locals.naming` block standardizes resource names across all modules (prefix/env/location).\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udea6 CI/CD & Security\\n\\n* **Pipeline matrix** per environment (Plan/Apply) with manual approval for prod.\\n* **Pre-commit**: `terraform fmt`, `tflint`, `tfsec`/`checkov`, `terrascan`.\\n* **Drift Detection**: `terraform plan` on schedule \u2192 Slack/Teams\u2011Report.\\n* Sign/archive **Plan\u2011Artifacts**.\\n* **Provider\u2011Pins** + Renovate/Bump PRs \u2192 reproducible builds.\\n\\n**Pattern:** `make plan ENV=dev` calls `terraform workspace select dev` + `-var-file=env/dev/variables.tfvars` \u2013 identical commands locally and in CI.\\n\\n---\\n\\n## \ud83d\udce6 Complete AKS example (abridged)\\n\\n```hcl\\nmodule \\"network\\" {\\n  source              = \\"../modules/network\\"\\n  name                = local.naming.net\\n  location            = var.location\\n  address_space       = [\\"10.40.0.0/16\\"]\\n  subnets = {\\n    aks_nodes = {\\n      prefix = \\"10.40.1.0/24\\"\\n      nsg_rules = [\\"deny_internet_in\\", \\"allow_vnet\\"]\\n    }\\n  }\\n}\\n\\nmodule \\"acr\\" {\\n  source              = \\"../modules/acr\\"\\n  name                = local.naming.acr\\n  location            = var.location\\n  sku                 = \\"Standard\\"\\n}\\n\\nmodule \\"aks\\" {\\n  source                  = \\"../modules/aks\\"\\n  name                    = local.naming.aks\\n  location                = var.location\\n  resource_group_name     = azurerm_resource_group.rg.name\\n  kubernetes_version      = var.k8s_version\\n  network_profile = {\\n    plugin         = \\"azure\\"\\n    service_cidr   = \\"10.41.0.0/16\\"\\n    dns_service_ip = \\"10.41.0.10\\"\\n    outbound_type  = \\"managedNATGateway\\"\\n  }\\n  system_node_pool = {\\n    name      = \\"sys\\"\\n    vm_size   = \\"Standard_D4s_v5\\"\\n    min_count = 1\\n    max_count = 3\\n  }\\n  user_node_pools = [\\n    {\\n      name = \\"user\\"\\n      vm_size = \\"Standard_D8s_v5\\"\\n      min_count = 2\\n      max_count = 10\\n      labels = { \\"kubernetes.azure.com/mode\\" = \\"user\\" }\\n    }\\n  ]\\n  enable_azure_rbac            = true\\n  aad_admin_group_object_ids   = var.aad_admin_groups\\n}\\n\\n# Role Assignment for Kubelet \u2192 ACR Pull\\nresource \\"azurerm_role_assignment\\" \\"kubelet_acr\\" {\\n  scope                = module.acr.id\\n  role_definition_name = \\"AcrPull\\"\\n  principal_id         = module.aks.kubelet_identity_principal_id\\n}\\n```\\n\\n---\\n\\n## \ud83e\udded Checklist \u2013 Things that can go wrong\\n\\n* [ ] Forgot `AcrPull` for **Kubelet** \u2192 ImagePullBackOff\\n* [ ] Subnet/RT/NAT permissions missing \u2192 AKS provisioning hangs\\n* [ ] Azure RBAC groups not propagated \u2192 Admins cannot join (wait/retry)\\n* [ ] Network policies not taking effect (policy engine/CNI does not match cluster config)\\n* [ ] Private DNS not configured \u2192 Control plane/ingress/ACR not reachable\\n* [ ] Provider upgrade without module encapsulation \u2192 Breaking changes everywhere\\n\\n<Admonition type=\\"caution\\" title=\\"Production Readiness\\">\\nBefore prod rollout: e2e tests (deployments, pull from ACR, ingress, DNS, policy smoke), load tests, failover (node drain, pool scaling), backup/restore (etcd/Velero), secrets path (Key Vault + CSI).\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nA **modular Terraform design** for AKS pays off: clearer responsibilities, less drift, reproducible builds, and controlled security. With clean RBAC, a well-thought-out network layout, and automated checks, the platform remains **scalable** and **operationally stable**.\\n\\n---\\n\\n## \ud83d\udce6 Complete AKS module (example)\\n\\n**`modules/aks/main.tf`**\\n\\n```hcl\\nresource \\"azurerm_kubernetes_cluster\\" \\"this\\" {\\n  name                = var.name\\n  location            = var.location\\n  resource_group_name = var.resource_group_name\\n  dns_prefix          = \\"${var.name}-dns\\"\\n  kubernetes_version  = var.kubernetes_version\\n\\n  identity {\\n    type = \\"SystemAssigned\\"\\n  }\\n\\n  default_node_pool {\\n    name                = var.system_node_pool.name\\n    vm_size             = var.system_node_pool.vm_size\\n    min_count           = var.system_node_pool.min_count\\n    max_count           = var.system_node_pool.max_count\\n    enable_auto_scaling = true\\n    os_sku              = var.system_node_pool.os_sku\\n    mode                = \\"System\\"\\n  }\\n\\n  dynamic \\"agent_pool_profile\\" {\\n    for_each = var.user_node_pools\\n    content {\\n      name                = agent_pool_profile.value.name\\n      vm_size             = agent_pool_profile.value.vm_size\\n      min_count           = agent_pool_profile.value.min_count\\n      max_count           = agent_pool_profile.value.max_count\\n      enable_auto_scaling = true\\n      mode                = lookup(agent_pool_profile.value, \\"mode\\", \\"User\\")\\n      node_labels         = lookup(agent_pool_profile.value, \\"labels\\", null)\\n      node_taints         = lookup(agent_pool_profile.value, \\"taints\\", null)\\n    }\\n  }\\n\\n  role_based_access_control_enabled = var.enable_azure_rbac\\n\\n  azure_active_directory_role_based_access_control {\\n    managed                = true\\n    admin_group_object_ids = var.aad_admin_group_object_ids\\n  }\\n\\n  network_profile {\\n    network_plugin     = var.network_profile.plugin\\n    service_cidr       = var.network_profile.service_cidr\\n    dns_service_ip     = var.network_profile.dns_service_ip\\n    pod_cidr           = try(var.network_profile.pod_cidr, null)\\n    outbound_type      = var.network_profile.outbound_type\\n  }\\n}\\n\\noutput \\"kubelet_identity_principal_id\\" {\\n  value = azurerm_kubernetes_cluster.this.kubelet_identity[0].object_id\\n}\\n\\noutput \\"id\\" {\\n  value = azurerm_kubernetes_cluster.this.id\\n}\\n```\\n\\n**`modules/aks/variables.tf`**\\n\\n```hcl\\nvariable \\"name\\" { type = string }\\nvariable \\"location\\" { type = string }\\nvariable \\"resource_group_name\\" { type = string }\\nvariable \\"kubernetes_version\\" { type = string }\\n\\nvariable \\"network_profile\\" {\\n  type = object({\\n    plugin         = string\\n    service_cidr   = string\\n    dns_service_ip = string\\n    pod_cidr       = optional(string)\\n    outbound_type  = string\\n  })\\n}\\n\\nvariable \\"enable_azure_rbac\\" { type = bool }\\nvariable \\"aad_admin_group_object_ids\\" { type = list(string) }\\n\\nvariable \\"system_node_pool\\" {\\n  type = object({\\n    name      = string\\n    vm_size   = string\\n    min_count = number\\n    max_count = number\\n    os_sku    = optional(string, \\"Ubuntu\\")\\n  })\\n}\\n\\nvariable \\"user_node_pools\\" {\\n  type = list(object({\\n    name      = string\\n    vm_size   = string\\n    min_count = number\\n    max_count = number\\n    mode      = optional(string, \\"User\\")\\n    labels    = optional(map(string))\\n    taints    = optional(list(string))\\n  }))\\n  default = []\\n}\\n```\\n\\n---\\n\\n## \ud83d\ude80 Azure DevOps Pipeline for Terraform AKS\\n\\n**`.azure-pipelines/terraform-aks.yml`**\\n\\n```yaml\\ntrigger:\\n  branches:\\n    include:\\n      - main\\n\\nvariables:\\n  TF_VERSION: \'1.7.5\'\\n  AZURE_SUBSCRIPTION: \'MyServiceConnection\'\\n  ENVIRONMENT: \'dev\'\\n\\nstages:\\n  - stage: validate\\n    displayName: \\"Terraform Validate & Lint\\"\\n    jobs:\\n      - job: lint\\n        pool:\\n          vmImage: \'ubuntu-latest\'\\n        steps:\\n          - task: UseTerraform@0\\n            inputs:\\n              terraformVersion: $(TF_VERSION)\\n          - script: |\\n              terraform fmt -check -recursive\\n              terraform init -backend=false\\n              terraform validate\\n            displayName: \\"Terraform fmt & validate\\"\\n          - script: |\\n              curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash\\n              tflint --recursive\\n            displayName: \\"Run TFLint\\"\\n\\n  - stage: plan\\n    displayName: \\"Terraform Plan\\"\\n    dependsOn: validate\\n    jobs:\\n      - job: plan\\n        pool:\\n          vmImage: \'ubuntu-latest\'\\n        steps:\\n          - task: UseTerraform@0\\n            inputs:\\n              terraformVersion: $(TF_VERSION)\\n          - task: TerraformCLI@0\\n            displayName: \\"Terraform Init\\"\\n            inputs:\\n              command: \'init\'\\n              backendType: \'azurerm\'\\n              backendServiceArm: $(AZURE_SUBSCRIPTION)\\n              ensureBackend: true\\n              workingDirectory: \'infra/env/$(ENVIRONMENT)\'\\n          - task: TerraformCLI@0\\n            displayName: \\"Terraform Plan\\"\\n            inputs:\\n              command: \'plan\'\\n              environmentServiceName: $(AZURE_SUBSCRIPTION)\\n              workingDirectory: \'infra/env/$(ENVIRONMENT)\'\\n              vars: |\\n                environment=$(ENVIRONMENT)\\n          - publish: $(System.DefaultWorkingDirectory)/infra/env/$(ENVIRONMENT)/tfplan\\n            artifact: tfplan\\n\\n  - stage: apply\\n    displayName: \\"Terraform Apply\\"\\n    dependsOn: plan\\n    condition: and(succeeded(), eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\'))\\n    jobs:\\n      - deployment: apply\\n        environment: aks-$(ENVIRONMENT)\\n        pool:\\n          vmImage: \'ubuntu-latest\'\\n        strategy:\\n          runOnce:\\n            deploy:\\n              steps:\\n                - task: UseTerraform@0\\n                  inputs:\\n                    terraformVersion: $(TF_VERSION)\\n                - download: current\\n                  artifact: tfplan\\n                - task: TerraformCLI@0\\n                  displayName: \\"Terraform Apply\\"\\n                  inputs:\\n                    command: \'apply\'\\n                    environmentServiceName: $(AZURE_SUBSCRIPTION)\\n                    workingDirectory: \'infra/env/$(ENVIRONMENT)\'\\n                    commandOptions: \\"tfplan\\"\\n```\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nWith a **clearly encapsulated AKS module** and a **CI/CD pipeline in Azure DevOps**, you get:\\n\\n* Reproducible cluster deployments\\n* Automated validation (fmt, validate, lint)\\n* Plan review with artifacts\\n* Manual or automated apply with service connection\\n* Easy extensibility (drift detection, security scans)\\n\\n---\\n\\n## \ud83e\udde9 Production-ready AKS module (Terraform)\\n\\n> Structure (as an example):\\n>\\n> ```text\\n> modules/aks/\\n> \u251c\u2500 main.tf\\n> \u251c\u2500 variables.tf\\n> \u251c\u2500 outputs.tf\\n> \u2514\u2500 README.md\\n> ```\\n\\n**`modules/aks/variables.tf`**\\n\\n```hcl\\nvariable \\"name\\" { type = string }\\nvariable \\"location\\" { type = string }\\nvariable \\"resource_group_name\\" { type = string }\\n\\nvariable \\"kubernetes_version\\" { type = string }\\n\\nvariable \\"tags\\" { type = map(string), default = {} }\\n\\nvariable \\"identity_type\\" {\\n  description = \\"system | user\\"\\n  type        = string\\n  default     = \\"system\\"\\n  validation {\\n    condition     = contains([\\"system\\", \\"user\\"], var.identity_type)\\n    error_message = \\"identity_type must be \'system\' or \'user\'\\"\\n  }\\n}\\n\\nvariable \\"user_assigned_identity_ids\\" {\\n  description = \\"Only used when identity_type=user\\"\\n  type        = list(string)\\n  default     = []\\n}\\n\\nvariable \\"network_profile\\" {\\n  type = object({\\n    plugin           = string                    # azure | kubenet | cni_overlay\\n    service_cidr     = string\\n    dns_service_ip   = string\\n    pod_cidr         = optional(string)\\n    outbound_type    = optional(string, \\"managedNATGateway\\") # loadBalancer | userDefinedRouting | managedNATGateway\\n    network_policy   = optional(string, null)   # azure | calico | null\\n  })\\n}\\n\\nvariable \\"private_cluster_enabled\\" { type = bool, default = false }\\nvariable \\"api_server_authorized_ip_ranges\\" { type = list(string), default = [] }\\n\\nvariable \\"aad_admin_group_object_ids\\" { type = list(string), default = [] }\\nvariable \\"enable_azure_rbac\\" { type = bool, default = true }\\n\\nvariable \\"oms_workspace_resource_id\\" { type = string, default = null }\\nvariable \\"enable_azure_policy_addon\\" { type = bool, default = false }\\n\\nvariable \\"system_node_pool\\" {\\n  type = object({\\n    name               = string\\n    vm_size            = string\\n    min_count          = number\\n    max_count          = number\\n    os_disk_size_gb    = optional(number, 128)\\n    os_sku             = optional(string, \\"Ubuntu\\")\\n    node_labels        = optional(map(string), {})\\n    node_taints        = optional(list(string), [])\\n    zones              = optional(list(string), null)\\n  })\\n}\\n\\nvariable \\"user_node_pools\\" {\\n  description = \\"List of additional user pools\\"\\n  type = list(object({\\n    name               = string\\n    vm_size            = string\\n    min_count          = number\\n    max_count          = number\\n    os_disk_size_gb    = optional(number, 128)\\n    os_sku             = optional(string, \\"Ubuntu\\")\\n    node_labels        = optional(map(string), {})\\n    node_taints        = optional(list(string), [])\\n    mode               = optional(string, \\"User\\")\\n    zones              = optional(list(string), null)\\n  }))\\n  default = []\\n}\\n```\\n\\n**`modules/aks/main.tf`**\\n\\n```hcl\\n# Note: Provider configuration (azurerm) is set outside the module.\\n\\nlocals {\\n  identity_block = var.identity_type == \\"user\\" ? {\\n    type         = \\"UserAssigned\\"\\n    identity_ids = var.user_assigned_identity_ids\\n  } : {\\n    type = \\"SystemAssigned\\"\\n  }\\n}\\n\\nresource \\"azurerm_kubernetes_cluster\\" \\"this\\" {\\n  name                = var.name\\n  location            = var.location\\n  resource_group_name = var.resource_group_name\\n  kubernetes_version  = var.kubernetes_version\\n  dns_prefix          = replace(var.name, \\".\\", \\"-\\")\\n\\n  identity {\\n    type         = local.identity_block.type\\n    identity_ids = try(local.identity_block.identity_ids, null)\\n  }\\n\\n  default_node_pool {\\n    name                 = var.system_node_pool.name\\n    vm_size              = var.system_node_pool.vm_size\\n    orchestrator_version = var.kubernetes_version\\n    min_count            = var.system_node_pool.min_count\\n    max_count            = var.system_node_pool.max_count\\n    enable_auto_scaling  = true\\n    os_sku               = var.system_node_pool.os_sku\\n    os_disk_size_gb      = var.system_node_pool.os_disk_size_gb\\n    node_labels          = var.system_node_pool.node_labels\\n    node_taints          = var.system_node_pool.node_taints\\n    zones                = var.system_node_pool.zones\\n    upgrade_settings {\\n      max_surge = \\"33%\\"\\n    }\\n  }\\n\\n  network_profile {\\n    network_plugin    = var.network_profile.plugin\\n    service_cidr      = var.network_profile.service_cidr\\n    dns_service_ip    = var.network_profile.dns_service_ip\\n    network_policy    = var.network_profile.network_policy\\n    outbound_type     = var.network_profile.outbound_type\\n    pod_cidr          = try(var.network_profile.pod_cidr, null)\\n  }\\n\\n  api_server_access_profile {\\n    authorized_ip_ranges = var.api_server_authorized_ip_ranges\\n  }\\n\\n  azure_active_directory_role_based_access_control {\\n    enabled                        = true\\n    azure_rbac_enabled             = var.enable_azure_rbac\\n    admin_group_object_ids         = var.aad_admin_group_object_ids\\n  }\\n\\n  # Add-ons\\n  dynamic \\"oms_agent\\" {\\n    for_each = var.oms_workspace_resource_id == null ? [] : [1]\\n    content {\\n      log_analytics_workspace_id = var.oms_workspace_resource_id\\n    }\\n  }\\n\\n  azure_policy_enabled        = var.enable_azure_policy_addon\\n  private_cluster_enabled     = var.private_cluster_enabled\\n\\n  sku_tier = \\"Paid\\" # Uptime SLA optional, anpassbar\\n\\n  tags = var.tags\\n}\\n\\n# Additional User Node Pools\\nresource \\"azurerm_kubernetes_cluster_node_pool\\" \\"user\\" {\\n  for_each = { for p in var.user_node_pools : p.name => p }\\n\\n  kubernetes_cluster_id = azurerm_kubernetes_cluster.this.id\\n  name                  = each.value.name\\n  vm_size               = each.value.vm_size\\n  orchestrator_version  = var.kubernetes_version\\n  mode                  = try(each.value.mode, \\"User\\")\\n  min_count             = each.value.min_count\\n  max_count             = each.value.max_count\\n  enable_auto_scaling   = true\\n  os_disk_size_gb       = try(each.value.os_disk_size_gb, 128)\\n  os_sku                = try(each.value.os_sku, \\"Ubuntu\\")\\n  node_labels           = try(each.value.node_labels, {})\\n  node_taints           = try(each.value.node_taints, [])\\n  zones                 = try(each.value.zones, null)\\n\\n  tags = var.tags\\n}\\n```\\n\\n**`modules/aks/outputs.tf`**\\n\\n```hcl\\noutput \\"id\\" { value = azurerm_kubernetes_cluster.this.id }\\noutput \\"kubelet_identity_principal_id\\" {\\n  value = try(azurerm_kubernetes_cluster.this.kubelet_identity[0].object_id, null)\\n}\\noutput \\"principal_id\\" { # Cluster (control plane) MI bei SystemAssigned\\n  value = try(azurerm_kubernetes_cluster.this.identity[0].principal_id, null)\\n}\\noutput \\"host\\" { value = azurerm_kubernetes_cluster.this.kube_config[0].host }\\noutput \\"name\\" { value = azurerm_kubernetes_cluster.this.name }\\n```\\n\\n**`modules/aks/README.md`** (Short)\\n\\n```md\\nEncapsulate inputs AKS details (RBAC, network, private cluster). Configure providers externally. Set role assignments (ACR pull, subnet/route table) externally.```\\n\\n---\\n\\n## \ud83d\udd01 Azure DevOps Pipeline (Terraform Plan/Apply, Multi\u2011Env)\\n\\n> Prerequisites\\n>\\n> * Azure DevOps **Service Connection** (ARM) with **Workload Identity/Federated Credentials** for Subscription/Resource Group.\\n> * Azure Storage backend for Terraform State (container + blob locking via lease).\\n> * Optional: Variable groups for `ARM_*`/backend parameters.\\n\\n**`azure-pipelines.yml`**\\n\\n```yaml\\ntrigger:\\n  branches:\\n    include: [ main ]\\npr:\\n  branches:\\n    include: [ main, feature/* ]\\n\\nvariables:\\n  TF_VERSION: \'1.8.5\'\\n  PROVIDER_AZURERM: \'~> 3.113\'\\n  # Backend (can be set via variable group)\\n  TF_BACKEND_RG: \'rg-tfstate\'\\n  TF_BACKEND_SA: \'sttfstate1234\'\\n  TF_BACKEND_CONTAINER: \'tfstate\'\\n  TF_BACKEND_KEY: \'$(Build.Repository.Name).$(System.StageName).tfstate\'\\n\\nstages:\\n- stage: Validate\\n  displayName: \\"Validate & Security Checks\\"\\n  jobs:\\n  - job: validate\\n    pool: { vmImage: \'ubuntu-latest\' }\\n    steps:\\n    - checkout: self\\n    - task: Bash@3\\n      displayName: \\"Install Terraform $(TF_VERSION)\\"\\n      inputs:\\n        targetType: \'inline\'\\n        script: |\\n          curl -sLo tf.zip https://releases.hashicorp.com/terraform/$(TF_VERSION)/terraform_$(TF_VERSION)_linux_amd64.zip\\n          sudo unzip -o tf.zip -d /usr/local/bin\\n          terraform -version\\n    - task: Bash@3\\n      displayName: \\"Terraform fmt & init\\"\\n      env:\\n        ARM_USE_OIDC: true\\n      inputs:\\n        targetType: \'inline\'\\n        script: |\\n          cd infra/env/dev\\n          terraform init \\\\\\n            -backend-config=\\"resource_group_name=$(TF_BACKEND_RG)\\" \\\\\\n            -backend-config=\\"storage_account_name=$(TF_BACKEND_SA)\\" \\\\\\n            -backend-config=\\"container_name=$(TF_BACKEND_CONTAINER)\\" \\\\\\n            -backend-config=\\"key=$(TF_BACKEND_KEY)\\"\\n          terraform fmt -check -recursive\\n          terraform validate\\n    - task: Bash@3\\n      displayName: \\"tflint / tfsec\\"\\n      inputs:\\n        targetType: \'inline\'\\n        script: |\\n          curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash\\n          tflint --version\\n          tflint -f compact || true\\n          curl -sL https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash\\n          tfsec . || true\\n\\n- stage: Plan\\n  displayName: \\"Plan (Dev)\\"\\n  dependsOn: Validate\\n  jobs:\\n  - job: plan_dev\\n    displayName: \\"terraform plan dev\\"\\n    pool: { vmImage: \'ubuntu-latest\' }\\n    steps:\\n    - checkout: self\\n    - task: AzureCLI@2\\n      displayName: \\"Terraform init+plan (OIDC)\\"\\n      inputs:\\n        azureSubscription: \'AZURE-SP-WI\'   # Name of your service connection\\n        scriptType: bash\\n        scriptLocation: inlineScript\\n        inlineScript: |\\n          set -e\\n          cd infra/env/dev\\n          terraform init \\\\\\n            -backend-config=\\"resource_group_name=$(TF_BACKEND_RG)\\" \\\\\\n            -backend-config=\\"storage_account_name=$(TF_BACKEND_SA)\\" \\\\\\n            -backend-config=\\"container_name=$(TF_BACKEND_CONTAINER)\\" \\\\\\n            -backend-config=\\"key=$(TF_BACKEND_KEY)\\"\\n          terraform workspace select dev || terraform workspace new dev\\n          terraform plan -var-file=variables.tfvars -out=tfplan\\n    - publish: infra/env/dev/tfplan\\n      displayName: \\"Publish plan artifact\\"\\n      artifact: tfplan-dev\\n\\n- stage: Apply\\n  displayName: \\"Apply (Dev)\\"\\n  dependsOn: Plan\\n  condition: and(succeeded(), eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\'))\\n  jobs:\\n  - deployment: apply_dev\\n    displayName: \\"terraform apply dev\\"\\n    environment: dev # optional: protect environments with approvals\\n    strategy:\\n      runOnce:\\n        deploy:\\n          steps:\\n          - checkout: self\\n          - task: AzureCLI@2\\n            displayName: \\"Terraform init+apply\\"\\n            inputs:\\n              azureSubscription: \'AZURE-SP-WI\'\\n              scriptType: bash\\n              scriptLocation: inlineScript\\n              inlineScript: |\\n                set -e\\n                cd infra/env/dev\\n                terraform init \\\\\\n                  -backend-config=\\"resource_group_name=$(TF_BACKEND_RG)\\" \\\\\\n                  -backend-config=\\"storage_account_name=$(TF_BACKEND_SA)\\" \\\\\\n                  -backend-config=\\"container_name=$(TF_BACKEND_CONTAINER)\\" \\\\\\n                  -backend-config=\\"key=$(TF_BACKEND_KEY)\\"\\n                terraform workspace select dev || terraform workspace new dev\\n                terraform apply -auto-approve tfplan\\n\\n- stage: Plan_Prod\\n  displayName: \\"Plan (Prod)\\"\\n  dependsOn: Apply\\n  condition: and(succeeded(), eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\'))\\n  jobs:\\n  - job: plan_prod\\n    pool: { vmImage: \'ubuntu-latest\' }\\n    steps:\\n    - checkout: self\\n    - task: AzureCLI@2\\n      displayName: \\"Plan prod\\"\\n      inputs:\\n        azureSubscription: \'AZURE-SP-WI\'\\n        scriptType: bash\\n        scriptLocation: inlineScript\\n        inlineScript: |\\n          set -e\\n          cd infra/env/prod\\n          terraform init \\\\\\n            -backend-config=\\"resource_group_name=$(TF_BACKEND_RG)\\" \\\\\\n            -backend-config=\\"storage_account_name=$(TF_BACKEND_SA)\\" \\\\\\n            -backend-config=\\"container_name=$(TF_BACKEND_CONTAINER)\\" \\\\\\n            -backend-config=\\"key=$(TF_BACKEND_KEY)\\"\\n          terraform workspace select prod || terraform workspace new prod\\n          terraform plan -var-file=variables.tfvars -out=tfplan\\n    - publish: infra/env/prod/tfplan\\n      artifact: tfplan-prod\\n\\n- stage: Apply_Prod\\n  displayName: \\"Apply (Prod)\\"\\n  dependsOn: Plan_Prod\\n  condition: and(succeeded(), eq(variables[\'Build.SourceBranch\'], \'refs/heads/main\'))\\n  jobs:\\n  - deployment: apply_prod\\n    displayName: \\"terraform apply prod\\"\\n    environment: prod # Enforce manual approval in ADO Environment\\n    strategy:\\n      runOnce:\\n        deploy:\\n          steps:\\n          - checkout: self\\n          - task: AzureCLI@2\\n            displayName: \\"Apply prod\\"\\n            inputs:\\n              azureSubscription: \'AZURE-SP-WI\'\\n              scriptType: bash\\n              scriptLocation: inlineScript\\n              inlineScript: |\\n                set -e\\n                cd infra/env/prod\\n                terraform init \\\\\\n                  -backend-config=\\"resource_group_name=$(TF_BACKEND_RG)\\" \\\\\\n                  -backend-config=\\"storage_account_name=$(TF_BACKEND_SA)\\" \\\\\\n                  -backend-config=\\"container_name=$(TF_BACKEND_CONTAINER)\\" \\\\\\n                  -backend-config=\\"key=$(TF_BACKEND_KEY)\\"\\n                terraform workspace select prod || terraform workspace new prod\\n                terraform apply -auto-approve tfplan\\n```\\n\\n### Notes & Best Practices\\n\\n* **OIDC/Federated Credentials:** Configure Service Connection so that no secret is required (no service principal password in the repo).\\n* **State per Stage:** The key `$(System.StageName)` in the backend cleanly separates dev/prod.\\n* **Security Scans:** `tflint`/`tfsec` are `|| true` so that warnings do not hard break the build \u2013 optionally enforce in prod.\\n* **Approvals:** Use Azure DevOps **Environments** for manual approvals between stages.\\n* **Parallel Envs:** For multiple envs, use `strategy: matrix` in Plan/Apply or define envs as separate stages.\\n\\n---\\n\\n## \ud83d\udd17 Example: Using the AKS module in `infra/env/dev/main.tf`\\n\\n```hcl\\nterraform {\\n  required_version = \\">= 1.8.0\\"\\n  required_providers {\\n    azurerm = {\\n      source  = \\"hashicorp/azurerm\\"\\n      version = \\"~> 3.113\\"\\n    }\\n  }\\n  backend \\"azurerm\\" {}\\n}\\n\\nprovider \\"azurerm\\" {\\n  features {}\\n  use_oidc = true\\n}\\n\\nlocals {\\n  tags = {\\n    env   = \\"dev\\"\\n    owner = \\"platform\\"\\n  }\\n}\\n\\nresource \\"azurerm_resource_group\\" \\"rg\\" {\\n  name     = \\"rg-aks-dev\\"\\n  location = var.location\\n  tags     = local.tags\\n}\\n\\nmodule \\"aks\\" {\\n  source              = \\"../../modules/aks\\"\\n  name                = \\"aks-dev-core\\"\\n  location            = var.location\\n  resource_group_name = azurerm_resource_group.rg.name\\n  kubernetes_version  = var.k8s_version\\n  tags                = local.tags\\n\\n  identity_type = \\"system\\"\\n\\n  network_profile = {\\n    plugin         = \\"azure\\"\\n    service_cidr   = \\"10.50.0.0/16\\"\\n    dns_service_ip = \\"10.50.0.10\\"\\n    outbound_type  = \\"managedNATGateway\\"\\n    network_policy = \\"azure\\"\\n  }\\n\\n  aad_admin_group_object_ids = var.aad_admin_groups\\n  enable_azure_rbac          = true\\n\\n  private_cluster_enabled           = false\\n  api_server_authorized_ip_ranges   = []\\n\\n  system_node_pool = {\\n    name      = \\"sys\\"\\n    vm_size   = \\"Standard_D4s_v5\\"\\n    min_count = 1\\n    max_count = 2\\n    node_labels = {\\n      \\"kubernetes.azure.com/mode\\" = \\"system\\"\\n    }\\n  }\\n\\n  user_node_pools = [\\n    {\\n      name      = \\"user\\"\\n      vm_size   = \\"Standard_D8s_v5\\"\\n      min_count = 2\\n      max_count = 6\\n      node_labels = {\\n        \\"kubernetes.azure.com/mode\\" = \\"user\\"\\n      }\\n    }\\n  ]\\n}\\n\\n# Example: ACR + Role Assignment (outside the module)\\nresource \\"azurerm_container_registry\\" \\"acr\\" {\\n  name                = \\"acrdevexample1234\\"\\n  resource_group_name = azurerm_resource_group.rg.name\\n  location            = var.location\\n  sku                 = \\"Standard\\"\\n  admin_enabled       = false\\n  tags                = local.tags\\n}\\n\\nresource \\"azurerm_role_assignment\\" \\"kubelet_acr_pull\\" {\\n  scope                = azurerm_container_registry.acr.id\\n  role_definition_name = \\"AcrPull\\"\\n  principal_id         = module.aks.kubelet_identity_principal_id\\n}\\n```"},{"id":"aks-node-selection","metadata":{"permalink":"/en/blog/aks-node-selection","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025-01-27-aks-node-selection.md","title":"AKS Node Selection: Physical Pools vs. Virtual Nodes","description":"Strategies for running workloads in AKS preferentially on physical user nodes \u2013 with automatic fallback to virtual nodes.","date":"2025-01-27T00:00:00.000Z","tags":[{"inline":false,"label":"Kubernetes","permalink":"/en/blog/tags/kubernetes","description":"Kubernetes"},{"inline":false,"label":"AKS","permalink":"/en/blog/tags/aks","description":"Azure Kubernetes Service"},{"inline":false,"label":"Azure","permalink":"/en/blog/tags/azure","description":"Azure"},{"inline":false,"label":"Nodepool","permalink":"/en/blog/tags/nodepool","description":"Nodepool"},{"inline":false,"label":"Virtual Node","permalink":"/en/blog/tags/virtual-node","description":"Virtual Node"},{"inline":false,"label":"Scheduling","permalink":"/en/blog/tags/scheduling","description":"Scheduling"}],"readingTime":4.59,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"aks-node-selection","title":"AKS Node Selection: Physical Pools vs. Virtual Nodes","authors":"brigitte","tags":["kubernetes","aks","azure","nodepool","virtual-node","scheduling"],"date":"2025-01-27T00:00:00.000Z","description":"Strategies for running workloads in AKS preferentially on physical user nodes \u2013 with automatic fallback to virtual nodes."},"unlisted":false,"prevItem":{"title":"Terraform Patterns for AKS & Azure","permalink":"/en/blog/terraform-patterns-aks-azure"},"nextItem":{"title":"Streaming File Uploads to Azure Blob Storage with Spring Boot","permalink":"/en/blog/springboot-fileupload-azure"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nIn many projects, the **cost and resource model** is crucial:\\n- Physical AKS node pools (`user nodes`) are cheaper and optimized for continuous workloads.  \\n- **Virtual nodes** (based on Azure Container Instances) are ideal for **burst scenarios**\u2014when more capacity is needed at short notice.  \\n\x3c!--truncate--\x3e\\n\ud83d\udc49 Goal: Workloads should **always use the physical nodes first**, but automatically switch to virtual nodes when there are no more resources available there.\\n\\n---\\n\\n## \u2699\ufe0f Basics: Node pools in AKS\\n- **System Pool**: internal AKS services\\n- **User Pool**: physical VM-based nodes (e.g., VMSS with Standard_D4s_v5)\\n- **Virtual Node Pool**: based on ACI, highly scalable, pay-per-use, no VM instance costs\\n\\n---\\n\\n## \ud83d\udea7 Challenge\\nBy default, Kubernetes distributes pods evenly \u2013 without \u201cpreference.\\"  \\nIf you want to use virtual nodes **only as a stopgap measure**, you need a clean scheduling strategy.\\n\\n---\\n\\n## \u2705 Strategies for Node Selection\\n\\n### 1. NodeSelector + Taints/Tolerations\\n- User nodes: no special taints \u2192 pods run here by default.  \\n- Virtual nodes: tainted (`virtual-kubelet.io/provider=azure:NoSchedule`).  \\n- Only pods that set **tolerations** are allowed to fall back to virtual nodes.\\n\\n```yaml\\ntolerations:\\n  - key: \u201cvirtual-kubelet.io/provider\\"\\n    operator: \u201cEqual\\"\\n    value: \u201cazure\\"\\n    effect: \u201cNoSchedule\\"\\n````\\n\\n\u27a1\ufe0f Advantage: full control, default = user nodes, virtual nodes = fallback.\\n\\n---\\n\\n2. Affinity & Preferred Scheduling\\n\\n`nodeAffinity` can be used to express a **preference**:\\n\\n* \u201cPrefer user nodes\\" (preferred)\\n* \u201cAllow virtual nodes\\" (soft)\\n\\n```yaml\\naffinity:\\n  nodeAffinity:\\n    preferredDuringSchedulingIgnoredDuringExecution:\\n      - weight: 100\\n        preference:\\n          matchExpressions:\\n            - key: kubernetes.azure.com/mode\\n              operator: In\\n              values:\\n                - user\\n```\\n\\n\ud83d\udc49 Only when there is no more space there will pods be distributed to other nodes (including virtual nodes).\\n\\n---\\n\\n### 3. Workload-specific control\\n\\n* **Batch/burst jobs**: Set `tolerations` so that they can use virtual nodes.\\n* **Persistent services**: No taint/toleration \u2192 remain strictly on physical nodes.\\n\\n---\\n\\n## \ud83d\udcca Visualization: Scheduling Strategy\\n\\n```mermaid\\nflowchart TD\\n    subgraph AKS[\u201cAKS Cluster\\"]\\n        subgraph UserPool[\u201cUser Node Pool (VMs)\\"]\\n            U1[\u201cUser Node 1\\"]\\n            U2[\u201cUser Node 2\\"]\\n            U3[\u201cUser Node 3\\"]\\n        end\\n\\n        subgraph VirtualPool[\u201cVirtual Node Pool (ACI)\\"]\\n            V1[\u201cVirtual Node\\"]\\n        end\\n\\n        P1[\u201cPod A (Deployment)\\"]\\n        P2[\u201cPod B (Job)\\"]\\n    end\\n\\n    P1 --\x3e|preferred| U1 & U2 & U3\\n    P1 -.->|fallback| V1\\n\\n    P2 --\x3e|toleration| V1\\n```\\n\\n* **Pod A (Deployment)**: prefers user nodes, but falls back to virtual nodes when resources are scarce.\\n* **Pod B (Job)**: has explicit tolerance \u2192 may run directly on virtual nodes.\\n\\n---\\n\\n## \ud83d\udccc Best Practices\\n\\n* **Monitoring**: Track exactly how many pods are running on virtual nodes (cost control).\\n* **SLA**: Virtual nodes have different limits (no DaemonSet support, limited features).\\n* **Workload design**: Short jobs and burst-like loads \u2192 virtual nodes; critical systems \u2192 user nodes.\\n* **Cost model**: Physical pools for base load, Virtual Nodes only for peaks.\\n\\n<Admonition type=\\"tip\\" title=\\"Avoid cost traps\\">\\nSet limits and autoscaling correctly, otherwise too many pods will end up permanently on expensive Virtual Nodes!\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nWith **Affinity, Taints & Tolerations**, two-stage scheduling can be implemented:\\n\\n* Permanent workloads run reliably and cost-effectively on physical user nodes.\\n* Peak loads are automatically directed to virtual nodes \u2013 flexibly, scalably, and without overprovisioning.\\n\\n---\\n\\n## \ud83d\udce6 Example manifests (deployment & batch job)\\n\\n> Assumptions:\\n>\\n> * **User nodes** carry the label: `kubernetes.azure.com/mode=user`\\n> * **Virtual nodes** are tainted with `virtual-kubelet.io/provider=azure:NoSchedule`\\n> * Cluster has at least one Linux virtual node (ACI)\\n\\n### 1) Deployment: prefers user nodes, fallback to virtual node\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: web-frontend\\n  labels:\\n    app: web-frontend\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: web-frontend\\n  template:\\n    metadata:\\n      labels:\\n        app: web-frontend\\n    spec:\\n      # \u2776 Prefer physical user nodes\\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n            - weight: 100\\n              preference:\\n                matchExpressions:\\n                  - key: kubernetes.azure.com/mode\\n                    operator: In\\n                    values: [\\"user\\"]\\n      # \u2777 Allow fallback to virtual nodes (tolerate taint)\\n      tolerations:\\n        - key: \\"virtual-kubelet.io/provider\\"\\n          operator: \\"Equal\\"\\n          value: \\"azure\\"\\n          effect: \\"NoSchedule\\"\\n      # \u2778 Optional: Distribute pods across user nodes (cost & resilience)\\n      topologySpreadConstraints:\\n        - maxSkew: 1\\n          topologyKey: kubernetes.io/hostname\\n          whenUnsatisfiable: ScheduleAnyway\\n          labelSelector:\\n            matchLabels:\\n              app: web-frontend\\n      containers:\\n        - name: app\\n          image: ghcr.io/example/web:1.2.3\\n          ports:\\n            - containerPort: 8080\\n          resources:\\n            requests:\\n              cpu: \\"250m\\"\\n              memory: \\"256Mi\\"\\n            limits:\\n              cpu: \\"1\\"\\n              memory: \\"512Mi\\"\\n```\\n\\n> Result: As long as resources are available on user nodes, all replicas are placed there. Only when resources are scarce can pods also be scheduled on virtual nodes thanks to **tolerance**.\\n\\n---\\n\\n### 2) BatchJob: prefers virtual nodes to conserve user pool\\n\\n```yaml\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: image-transcode\\nspec:\\n  completions: 5\\n  parallelism: 5\\n  backoffLimit: 0\\n  template:\\n    spec:\\n      # \u2776 Prefer Virtual Node (soft), but allow users as backup\\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n            - weight: 100\\n              preference:\\n                matchExpressions:\\n                  - key: kubernetes.io/role\\n                    operator: In\\n                    values: [\\"virtual-node\\"]\\n      # \u2777 Tolerance for the virtual node taint (required for scheduling)\\n      tolerations:\\n        - key: \\"virtual-kubelet.io/provider\\"\\n          operator: \\"Equal\\"\\n          value: \\"azure\\"\\n          effect: \\"NoSchedule\\"\\n      restartPolicy: Never\\n      containers:\\n        - name: worker\\n          image: ghcr.io/example/transcoder:2.0.0\\n          args: [\\"--input\\", \\"$(INPUT)\\", \\"--output\\", \\"$(OUTPUT)\\"]\\n          env:\\n            - name: INPUT\\n              value: \\"/data/in\\"\\n            - name: OUTPUT\\n              value: \\"/data/out\\"\\n          resources:\\n            requests:\\n              cpu: \\"1\\"\\n              memory: \\"1Gi\\"\\n            limits:\\n              cpu: \\"2\\"\\n              memory: \\"2Gi\\"\\n```\\n\\n> Note: The key `kubernetes.io/role=virtual-node` is **an example label**. In many clusters, a suitable label already exists on virtual nodes (e.g., `type=virtual-kubelet` or `kubernetes.azure.com/virtual-node=true`). Adjust the **match expression** to your actual node labels.\\n\\n---\\n\\n### 3) Variant: Strict separation via NodeSelector\\n\\nIf certain workloads should **never** run on virtual nodes, use a hard `nodeSelector` on user nodes **without** tolerations:\\n\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      nodeSelector:\\n        kubernetes.azure.com/mode: \u201cuser\\"\\n      # No toleration \u2192 no scheduling on virtual nodes possible\\n```\\n\\nAnd vice versa (virtual node only):\\n\\n```yaml\\nspec:\\n  template:\\n    spec:\\n      tolerations:\\n        - key: \\"virtual-kubelet.io/provider\\"\\n          operator: \\"Equal\\"\\n          value: \\"azure\\"\\n          effect: \\"NoSchedule\\"\\n      nodeSelector:\\n        kubernetes.azure.com/virtual-node: \\"true\\" # Sample label, customize it for your needs\\n```\\n\\n---\\n\\n### 4) Horizontal Pod Autoscaler (HPA) as a burst trigger\\n\\n```yaml\\napiVersion: autoscaling/v2\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  name: web-frontend\\nspec:\\n  scaleTargetRef:\\n    apiVersion: apps/v1\\n    kind: Deployment\\n    name: web-frontend\\n  minReplicas: 3\\n  maxReplicas: 30\\n  metrics:\\n    - type: Resource\\n      resource:\\n        name: cpu\\n        target:\\n          type: Utilization\\n          averageUtilization: 70\\n```"},{"id":"springboot-fileupload-azure","metadata":{"permalink":"/en/blog/springboot-fileupload-azure","source":"@site/i18n/en/docusaurus-plugin-content-blog/2025-01-20-springboot-fileupload-azure.md","title":"Streaming File Uploads to Azure Blob Storage with Spring Boot","description":"Memory-efficient processing of large uploads directly in Azure Storage\u2014without temporary storage in RAM.","date":"2025-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"Spring Boot","permalink":"/en/blog/tags/spring-boot","description":"Spring Boot"},{"inline":false,"label":"Kotlin","permalink":"/en/blog/tags/kotlin","description":"Kotlin"},{"inline":false,"label":"Java","permalink":"/en/blog/tags/java","description":"Java"},{"inline":false,"label":"Azure","permalink":"/en/blog/tags/azure","description":"Azure"},{"inline":false,"label":"Blob Storage","permalink":"/en/blog/tags/blob-storage","description":"Blob Storage"},{"inline":false,"label":"Fileupload","permalink":"/en/blog/tags/fileupload","description":"Fileupload"}],"readingTime":8.11,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"springboot-fileupload-azure","title":"Streaming File Uploads to Azure Blob Storage with Spring Boot","authors":"brigitte","tags":["spring-boot","kotlin","java","azure","blob-storage","fileupload"],"date":"2025-01-20T00:00:00.000Z","description":"Memory-efficient processing of large uploads directly in Azure Storage\u2014without temporary storage in RAM."},"unlisted":false,"prevItem":{"title":"AKS Node Selection: Physical Pools vs. Virtual Nodes","permalink":"/en/blog/aks-node-selection"},"nextItem":{"title":"Workload Identity in AKS \u2013 Lessons Learned","permalink":"/en/blog/workload-identity-lessons-learned"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nAnyone who wants to upload large files (several gigabytes) via a web application quickly reaches their limits:\\n- Classic multipart processing loads everything into memory or onto the disk.\\n- Uploads take a long time and block threads.\\n- Upload errors lead to inconsistent data states.\\n\x3c!-- truncate --\x3e\\nWith a **streaming-based approach**, files can be written directly to Azure Blob Storage during upload \u2013 without ever being cached in RAM or on disk.\\n\\n---\\n\\n\\n\\n## \u2699\ufe0f Setup\\n\\n- **Spring Boot + Kotlin** as a Basis\\n- [`commons-fileupload2-core`](https://commons.apache.org/proper/commons-fileupload/) for streaming multipart parsing\\n- **Azure Blob Storage SDK** for writing streams to blobs\\n- **SAS tokens** for scoped & time-limited access\\n\\n### Streaming Multipart Upload\\n\\n```kotlin\\nval iterator = FileUploadStreaming.getItemIterator(request)\\nwhile (iterator.hasNext()) {\\n    val item = iterator.next()\\n    if (!item.isFormField) {\\n        val blobClient = containerClient.getBlobClient(item.name)\\n        blobClient.getBlockBlobClient().upload(item.inputStream, item.size, true)\\n    }\\n}\\n```\\n\\n\ud83d\udc49 No file is stored on the disk or in the working memory \u2013 the InputStream is passed directly to Azure.\\n\\n---\\n\\n## \ud83d\udd0d Extension: Determining MIME type with Tika\\n\\nOften, the `Content-Type` provided by the client is not sufficient. To determine the **actual MIME type**, a **Custom InputStream** can be used, which caches the first bytes so that [Apache Tika](https://tika.apache.org/) can perform recognition:\\n\\n```kotlin\\nclass TikaInputStream(private val source: InputStream) : InputStream() {\\n    private val buffer = ByteArrayOutputStream()\\n    private var replay: ByteArrayInputStream? = null\\n    private var probed = false\\n\\n    override fun read(): Int {\\n        val replayStream = replay\\n        return if (replayStream != null) {\\n            replayStream.read()\\n        } else {\\n            val b = source.read()\\n            if (!probed && b != -1) buffer.write(b)\\n            b\\n        }\\n    }\\n\\n    fun detectMimeType(): String {\\n        if (!probed) {\\n            probed = true\\n            val bytes = buffer.toByteArray()\\n            replay = ByteArrayInputStream(bytes)\\n            return Tika().detect(bytes)\\n        }\\n        return \\"application/octet-stream\\"\\n    }\\n}\\n```\\n\\n\u26a1 Advantage: MIME detection happens **in the stream** without having to read the entire file.\\n\\n---\\n\\n## \ud83d\udce6 On-the-fly compression\\n\\nFor certain data types, **on-the-fly compression** is worthwhile. This involves packing the upload stream directly into a `GZIPOutputStream` before it is transferred to Azure:\\n\\n```kotlin\\nval blobClient = containerClient.getBlobClient(\u201c${item.name}.gz\\")\\nblobClient.getBlockBlobClient().upload(\\n    GZIPOutputStream(item.inputStream),\\n    item.size, // unknown if necessary, then use -1 and chunked upload\\n    true)\\n\\n```\\n\\n* Saves a lot of storage space and bandwidth.\\n* Should be **optional** (e.g., depending on the MIME type from Tika).\\n* Caution with binary files (videos, images): compression usually does not offer any advantages here.\\n\\n---\\n\\n## \ud83d\udea7 Stumbling blocks\\n\\n* **Multipart parsing:** Streams must be closed reliably.\\n* **Content length:** Not always delivered by the client \u2192 possibly use chunked upload.\\n* **Error handling:** If the upload is interrupted, metadata may also need to be rolled back.\\n* **Tika + compression:** Perform recognition first, then compress if necessary.\\n\\n---\\n\\n## \u2705 Best practices\\n\\n* **Backpressure:** Never buffer uploads, but stream them through.\\n* **Separation of metadata & storage**: separate services for separate responsibilities.\\n* **SAS tokens**: generate with prefix scopes and short lifetime.\\n* **Combination of Tika + compression**: Only compress if it really makes sense.\\n\\n<Admonition type=\\"note\\" title=\\"Practical benefits\\">\\nWe use this technique in production systems to process terabyte-scale uploads in a high-performance, secure, and cost-optimized manner.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nStreaming uploads are **feasible and production-ready** in Spring Boot \u2013 and even more flexible thanks to MIME detection and optional on-the-fly compression.\\nThe result: **lower infrastructure costs, better performance, and greater robustness**.\\n\\n---\\n\\n> Complete, executable example: Streaming multipart with `commons-fileupload2-core`, MIME detection via Apache Tika, optional on-the-fly compression (GZIP), and upload directly to Azure Blob Storage via SAS \u2013 **without** RAM/disk buffers.\\n\\n---\\n\\n## Project-Setup (Gradle Kotlin DSL)\\n\\n**`build.gradle.kts`**\\n\\n```kotlin\\nplugins {\\n    id(\\"org.springframework.boot\\") version \\"3.3.0\\"\\n    id(\\"io.spring.dependency-management\\") version \\"1.1.5\\"\\n    kotlin(\\"jvm\\") version \\"1.9.24\\"\\n    kotlin(\\"plugin.spring\\") version \\"1.9.24\\"\\n}\\n\\ngroup = \\"com.example\\"\\nversion = \\"0.0.1\\"\\njava.sourceCompatibility = JavaVersion.VERSION_17\\n\\nrepositories { mavenCentral() }\\n\\ndependencies {\\n    implementation(\\"org.springframework.boot:spring-boot-starter-web\\")\\n\\n    // Azure Blob Storage SDK v12\\n    implementation(\\"com.azure:azure-storage-blob:12.26.0\\")\\n\\n    // Streaming Multipart Parsing\\n    implementation(\\"org.apache.commons:commons-fileupload2-core:2.0.0-M1\\")\\n\\n    // Apache Tika for MIME-Erkennung\\n    implementation(\\"org.apache.tika:tika-core:2.9.2\\")\\n\\n    // Jackson / Kotlin\\n    implementation(\\"com.fasterxml.jackson.module:jackson-module-kotlin\\")\\n    implementation(kotlin(\\"reflect\\"))\\n\\n    testImplementation(\\"org.springframework.boot:spring-boot-starter-test\\")\\n}\\n\\ntasks.test { useJUnitPlatform() }\\n```\\n\\n> **Note:** Update versions to the latest version if necessary.\\n\\n**`src/main/resources/application.yaml`**\\n\\n```yaml\\nserver:\\n  tomcat:\\n    max-swallow-size: -1 # prevents termination with large streams\\n    max-http-form-post-size: -1\\n\\nazure:\\n  storage:\\n    # Fully qualified SAS URL of the container, e.g.:\\n    # https://<account>.blob.core.windows.net/<container>?sv=...&sig=...\\n    containerSasUrl: ${AZURE_CONTAINER_SAS_URL:}\\n\\nupload:\\n  compression:\\n    enabled: true # global switch, can be overridden per request\\n```\\n\\n---\\n\\n## Configuration: Azure Blob Container Client\\n\\n**`src/main/kotlin/com/example/upload/AzureStorageConfig.kt`**\\n\\n```kotlin\\npackage com.example.upload\\n\\nimport com.azure.storage.blob.BlobContainerClient\\nimport com.azure.storage.blob.BlobContainerClientBuilder\\nimport org.springframework.boot.context.properties.ConfigurationProperties\\nimport org.springframework.context.annotation.Bean\\nimport org.springframework.context.annotation.Configuration\\n\\n@Configuration\\nclass AzureStorageConfig {\\n    @Bean\\n    @ConfigurationProperties(prefix = \\"azure.storage\\")\\n    fun azureStorageProps() = AzureStorageProps()\\n\\n    @Bean\\n    fun blobContainerClient(props: AzureStorageProps): BlobContainerClient =\\n        BlobContainerClientBuilder()\\n            .endpoint(props.containerSasUrl)\\n            .buildClient()\\n}\\n\\nclass AzureStorageProps {\\n    /** Full container SAS URL including token */\\n    lateinit var containerSasUrl: String\\n}\\n```\\n\\n---\\n\\n## Utility: PeekableInputStream + MIME detection (Tika)\\n\\n**`src/main/kotlin/com/example/upload/io/PeekableInputStream.kt`**\\n\\n```kotlin\\npackage com.example.upload.io\\n\\nimport java.io.BufferedInputStream\\nimport java.io.InputStream\\n\\n/**\\n * Wraps an InputStream, allows peek via mark/reset without reading the entire stream.\\n */\\nclass PeekableInputStream(source: InputStream, private val peekBufferSize: Int = 8192) : InputStream() {\\n    private val inBuf = if (source.markSupported()) source else BufferedInputStream(source, peekBufferSize)\\n\\n    override fun read(): Int = inBuf.read()\\n    override fun read(b: ByteArray, off: Int, len: Int): Int = inBuf.read(b, off, len)\\n    override fun close() = inBuf.close()\\n\\n    fun <T> peek(peekLen: Int = peekBufferSize, block: (ByteArray) -> T): T {\\n        inBuf.mark(peekLen)\\n        val buf = ByteArray(peekLen)\\n        val n = inBuf.read(buf)\\n        inBuf.reset()\\n        val slice = if (n <= 0) ByteArray(0) else buf.copyOf(n)\\n        return block(slice)\\n    }\\n}\\n```\\n\\n**`src/main/kotlin/com/example/upload/mime/MimeDetector.kt`**\\n\\n```kotlin\\npackage com.example.upload.mime\\n\\nimport com.example.upload.io.PeekableInputStream\\nimport org.apache.tika.Tika\\n\\nobject MimeDetector {\\n    private val tika = Tika()\\n\\n    fun detect(peekable: PeekableInputStream, fallback: String = \\"application/octet-stream\\"): String =\\n        peekable.peek { bytes ->\\n            val detected = runCatching { tika.detect(bytes) }.getOrNull()\\n            detected ?: fallback\\n        }\\n}\\n```\\n\\n---\\n\\n## Service: Streaming upload with optional on-the-fly GZIP compression\\n\\n**`src/main/kotlin/com/example/upload/UploadService.kt`**\\n\\n```kotlin\\npackage com.example.upload\\n\\nimport com.azure.storage.blob.BlobContainerClient\\nimport com.azure.storage.blob.specialized.BlockBlobClient\\nimport com.example.upload.io.PeekableInputStream\\nimport com.example.upload.mime.MimeDetector\\nimport org.apache.commons.fileupload2.core.FileItemInputIterator\\nimport org.apache.commons.fileupload2.core.FileUpload\\nimport org.apache.commons.fileupload2.core.FileUploadException\\nimport org.apache.commons.fileupload2.core.RequestContext\\nimport org.springframework.stereotype.Service\\nimport java.io.InputStream\\nimport java.util.zip.GZIPOutputStream\\n\\n@Service\\nclass UploadService(private val container: BlobContainerClient) {\\n\\n    data class UploadResult(val files: List<FileInfo>)\\n    data class FileInfo(\\n        val fieldName: String,\\n        val filename: String,\\n        val size: Long?,\\n        val mimeType: String,\\n        val compressed: Boolean,\\n        val blobName: String\\n    )\\n\\n    /**\\n     * Stream multipart files directly to Azure. No intermediate buffers/temp files.\\n     * @param request Spring/Servlet request adapter for FileUpload2\\n     * @param forceCompression Optional override (header/param)\\n     */\\n    fun handleStreamingUpload(request: RequestContext, forceCompression: Boolean? = null): UploadResult {\\n        try {\\n            val iter: FileItemInputIterator = FileUpload().getItemIterator(request)\\n            val uploaded = mutableListOf<FileInfo>()\\n\\n            while (iter.hasNext()) {\\n                val item = iter.next()\\n                if (item.isFormField) continue\\n\\n                val originalName = item.name ?: \\"upload.bin\\"\\n                val field = item.fieldName ?: \\"file\\"\\n                val size = item.headers?.getHeader(\\"Content-Length\\")?.toLongOrNull()\\n\\n                // Make input stream peek-capable\\n                val peekable = PeekableInputStream(item.inputStream)\\n                val mime = MimeDetector.detect(peekable)\\n\\n                val shouldCompress = forceCompression\\n                    ?: shouldCompressMime(mime)\\n\\n                val (blobName, compressed) = if (shouldCompress) {\\n                    val nameGz = \\"$originalName.gz\\"\\n                    uploadStream(peekable, nameGz, compress = true)\\n                    nameGz to true\\n                } else {\\n                    uploadStream(peekable, originalName, compress = false)\\n                    originalName to false\\n                }\\n\\n                uploaded += FileInfo(\\n                    fieldName = field,\\n                    filename = originalName,\\n                    size = size,\\n                    mimeType = mime,\\n                    compressed = compressed,\\n                    blobName = blobName\\n                )\\n            }\\n\\n            return UploadResult(uploaded)\\n        } catch (e: FileUploadException) {\\n            throw RuntimeException(\\"Multipart parsing failed\\", e)\\n        }\\n    }\\n\\n    private fun shouldCompressMime(mime: String): Boolean {\\n        // Heuristics: textual = compress\\n        if (mime.startsWith(\\"text/\\")) return true\\n        return mime in setOf(\\n            \\"application/json\\",\\n            \\"application/xml\\",\\n            \\"application/x-ndjson\\",\\n            \\"text/csv\\",\\n            \\"application/csv\\"\\n        )\\n    }\\n\\n    private fun uploadStream(input: InputStream, blobName: String, compress: Boolean) {\\n        val client: BlockBlobClient = container.getBlobClient(blobName).blockBlobClient\\n\\n        // For unknown length: write via OutputStream (no length required)\\n        client.getBlobOutputStream(true).use { blobOut ->\\n            if (compress) {\\n                GZIPOutputStream(blobOut).use { gz ->\\n                    input.copyTo(gz, DEFAULT_BUFFER)\\n                    // GZIPOutputStream .close() writes Footer\\n                }\\n            } else {\\n                input.copyTo(blobOut, DEFAULT_BUFFER)\\n            }\\n        }\\n    }\\n\\n    companion object { const val DEFAULT_BUFFER = 1024 * 1024 }\\n}\\n```\\n\\n> We use **`BlockBlobClient.getBlobOutputStream(overwrite = true)`** so that no content length is required. This keeps the upload completely streaming-based.\\n\\n---\\n\\n## Controller: Minimal API (pass through servlet request)\\n\\n**`src/main/kotlin/com/example/upload/UploadController.kt`**\\n\\n```kotlin\\npackage com.example.upload\\n\\nimport org.apache.commons.fileupload2.core.RequestContext\\nimport org.springframework.http.MediaType\\nimport org.springframework.web.bind.annotation.PostMapping\\nimport org.springframework.web.bind.annotation.RequestHeader\\nimport org.springframework.web.bind.annotation.RequestMapping\\nimport org.springframework.web.bind.annotation.RestController\\nimport jakarta.servlet.http.HttpServletRequest\\n\\n@RestController\\n@RequestMapping(\\"/api\\")\\nclass UploadController(private val service: UploadService) {\\n\\n    @PostMapping(\\"/upload\\", consumes = [MediaType.MULTIPART_FORM_DATA_VALUE])\\n    fun upload(\\n        request: HttpServletRequest,\\n        @RequestHeader(name = \\"x-compress\\", required = false) compressHeader: String?\\n    ): UploadService.UploadResult {\\n        val forceCompression: Boolean? = compressHeader?.let { it.equals(\\"true\\", ignoreCase = true) }\\n\\n        val ctx = object : RequestContext {\\n            override fun getContentType(): String = request.contentType\\n            override fun getContentLength(): Int = request.contentLength\\n            override fun getCharacterEncoding(): String? = request.characterEncoding\\n            override fun getInputStream() = request.inputStream\\n        }\\n\\n        return service.handleStreamingUpload(ctx, forceCompression)\\n    }\\n}\\n```\\n\\n---\\n\\n## Error handling & (optional) rollback example\\n\\n**Pattern:** Manage metadata and blobs separately. Write the blob first, then create the metadata \u2013 or vice versa, with a **compensating action**.\\n\\n```kotlin\\ntry {\\n    // 1) Blob/Upload\\n    val result = service.handleStreamingUpload(ctx)\\n\\n    // 2) Send metadata to backend\\n    metadataClient.createFor(result.files)\\n\\n    return result\\n} catch (ex: Exception) {\\n    // Rollback strategy: delete any metadata that may have been created\\n    runCatching { metadataClient.rollback() }\\n    throw ex\\n}\\n```\\n\\n---\\n\\n## Test with `curl`\\n\\n```bash\\ncurl -X POST \\"http://localhost:8080/api/upload\\" \\\\\\n  -H \\"x-compress: true\\" \\\\\\n  -F \\"file=@./sample.csv\\" \\\\\\n  -H \\"Expect:\\" # verhindert 100-continue Verz\xf6gerung\\n```\\n\\n---\\n\\n## Security & operational aspects (short checklist)\\n\\n* **SAS tokens**: prefix-scoped (target path only), short lifetime, only necessary permissions (manage write/create/delete separately).\\n* **Backpressure**: no buffers, no temporary files; Tomcat limits (see `application.yaml`).\\n* **Limits**: Set server and proxy timeouts (AGIC/APIM) high enough.\\n* **Observability**: Log upload duration, bytes, client IP, MIME, compression flag (without PII). Traces for error paths.\\n* **Validation**: Whitelist of permitted MIME types, max file size on the server side (cancel early), virus scan as needed.\\n\\n---\\n\\n## FAQ\\n\\n**How do I determine the blob content type/encoding?**\\nIf not compressed: set `Content-Type` via blob HTTP header/metadata. For GZIP: set `Content-Encoding: gzip`, optionally save original MIME as user metadata.\\n\\n**Example:**\\n\\n```kotlin\\nval block = container.getBlobClient(blobName).blockBlobClient\\nval headers = com.azure.storage.blob.models.BlobHttpHeaders()\\n    .setContentType(\\"application/json\\")\\n    .setContentEncoding(\\"gzip\\")\\nblock.setHttpHeaders(headers)\\n```\\n\\n> `setHttpHeaders` can be set after the upload (separate call) \u2013 or you can use `beginUpload`/`commitBlockList` with options.\\n\\n**How do I prevent RAM spikes?**\\nKeep buffers small (1\u20134 MB), `copyTo` buffer constant. No `ByteArrayOutputStream` accumulation.\\n\\n**Can I parallelize?**\\nFor pure streaming endpoints: rather no (no length). For large known files, `ParallelTransferOptions` can be useful for `upload(InputStream, length)`.\\n\\n---\\n\\n## End-to-end sequence (simplified steps)\\n\\n1. Client sends multipart \u2192 Server parses stream via FileUpload2.\\n2. MIME detection via Peek (Tika).\\n3. Optional GZIP \u2192 Stream is compressed on-the-fly.\\n4. BlobOutputStream writes directly to Azure.\\n5. Optional: Set HTTP header/metadata, call metadata service.\\n6. Error \u2192 Trigger compensation (rollback).\\n\\n---"},{"id":"workload-identity-lessons-learned","metadata":{"permalink":"/en/blog/workload-identity-lessons-learned","source":"@site/i18n/en/docusaurus-plugin-content-blog/2024-10-11-workload-identity.md","title":"Workload Identity in AKS \u2013 Lessons Learned","description":"Setup, pitfalls, and best practices from real projects","date":"2024-11-10T00:00:00.000Z","tags":[{"inline":false,"label":"Kubernetes","permalink":"/en/blog/tags/kubernetes","description":"Kubernetes"},{"inline":false,"label":"Azure","permalink":"/en/blog/tags/azure","description":"Azure"},{"inline":false,"label":"AKS","permalink":"/en/blog/tags/aks","description":"Azure Kubernetes Service"},{"inline":false,"label":"Security","permalink":"/en/blog/tags/security","description":"Security"},{"inline":false,"label":"Identity","permalink":"/en/blog/tags/identity","description":"Identity"},{"inline":false,"label":"Lessons Learned","permalink":"/en/blog/tags/lessons-learned","description":"Lessons Learned"}],"readingTime":1.71,"hasTruncateMarker":true,"authors":[{"name":"Brigitte B\xf6hm","title":"Cloud & Data Platform Engineer","url":"https://www.linkedin.com/in/brigitte-boehm-34b7a025","page":{"permalink":"/en/blog/authors/brigitte"},"socials":{"github":"https://github.com/bri-b-dev","linkedin":"https://www.linkedin.com/in/brigitte-boehm-34b7a025"},"imageURL":"https://github.com/bri-b-dev.png","key":"brigitte"}],"frontMatter":{"slug":"workload-identity-lessons-learned","title":"Workload Identity in AKS \u2013 Lessons Learned","authors":"brigitte","tags":["kubernetes","azure","aks","security","identity","lessons-learned"],"date":"2024-11-10T00:00:00.000Z","description":"Setup, pitfalls, and best practices from real projects"},"unlisted":false,"prevItem":{"title":"Streaming File Uploads to Azure Blob Storage with Spring Boot","permalink":"/en/blog/springboot-fileupload-azure"}},"content":"import Admonition from \'@theme/Admonition\';\\n\\nWorkload Identity in **Azure Kubernetes Service (AKS)** promises fewer secrets, native AzureAD integration, and a replacement for the old AAD Pod Identity.  \\nIn practice, my projects in late 2024 brought not only smooth deployments but also some unexpected pitfalls.  \\n\x3c!-- truncate --\x3e\\nHere are my **hands-on experiences** \u2013 structured into Setup, Pitfalls, and Best Practices.\\n\\n---\\n\\n## \u2699\ufe0f Setup\\n\\n- **Cluster**: AKS with **Workload Identity feature** enabled  \\n- **Operator**: `azure-workload-identity` Admission Webhook in the cluster  \\n- **Service Accounts**: per pod with annotations like  \\n  ```yaml\\n  annotations:\\n    azure.workload.identity/client-id: <client-id>\\n  ```\\n\\n* **Azure side**:\\n\\n  * Managed Identities for pods\\n  * Role assignments via AAD & Azure RBAC (Storage, Key Vault, Application Gateway)\\n\\n---\\n\\n## \ud83d\udea7 Pitfalls\\n\\n### Sidecar injection not always reliable\\n\\nThe annotation `azure.workload.identity/inject-proxy-sidecar` didn\'t consistently work across operator versions.\\nSometimes special Helm templates or additional MutatingWebhook config were required.\\n\\n### AuthorizationPermissionMismatch\\n\\nA frequent error when accessing Storage Accounts.\\nRoot cause: mixing up **Management Plane** and **Data Plane** roles.\\n\u27a1\ufe0f Only proper Data Plane roles allow access.\\n\\n### Helm templates & securityContext\\n\\nAn incorrect `securityContext` in the Helm chart blocked sidecar injection.\\nDebugging this took time \u2013 webhook pod logs revealed the root cause.\\n\\n### Operator versions with breaking changes\\n\\nMinor operator releases sometimes changed behavior.\\n\u27a1\ufe0f Always check release notes before upgrading.\\n\\n---\\n\\n## \u2705 Best Practices\\n\\n* **Start small**: test with a single pod + storage account\\n* **Separate RBAC**: clearly distinguish management vs. data plane roles\\n* **Check operator logs**: Admission webhook is the first stop for debugging\\n* **Validate Helm templates**: ensure annotations and sidecars land in pod manifests\\n* **Plan time**: expect iterations during rollout\\n\\n<Admonition type=\\"tip\\" title=\\"My Tip\\">\\nWorkload Identity saves effort and increases security in the long run.  \\nBut allow **extra iterations** during introduction \u2013 especially with complex Helm charts.\\n</Admonition>\\n\\n---\\n\\n## \ud83d\udccc Conclusion\\n\\nWorkload Identity is a **key step for security and cloud-native architectures**.\\nThe initial rollout was not frictionless, but today our platform components run stable and secret-free.\\nIt\'s worth it \u2013 even if debugging took more effort than the docs suggested.\\n\\n---\\n\\n*Have you faced similar issues with AKS Workload Identity? Feel free to reach out or connect on [LinkedIn](https://www.linkedin.com/in/my-profile/).*"}]}}')}}]);